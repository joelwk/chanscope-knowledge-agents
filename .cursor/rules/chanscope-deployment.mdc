---
description: These rules define the operational requirements for containerized deployment of the Chanscope application, ensuring that Docker configurations properly support the data ingestion, processing, and query pipeline defined in the Chanscope approach.
globs: **/docker-compose*.yml,*/Docker*,**/deployment
alwaysApply: false
---

# Chanscope Deployment Rules

This file provides guidelines for Chanscope deployment with Docker and Replit, ensuring alignment between infrastructure components and the core Chanscope approach.

## Production Deployment Requirements

### 1. Volume Configuration

- **Data Persistence**: The Docker setup must maintain persistent volumes for:
  - `/app/data` - For storing complete and stratified data
  - `/app/logs` - For application logs
  - `/app/temp_files` - For temporary processing files

- **Volume Ownership**: Volumes must be accessible to the non-root user (`nobody:nogroup`) running the container.

### 2. Environment Configuration

- **Data Retention**: `DATA_RETENTION_DAYS` must be set appropriately (default: 14).
- **Data Scheduler**: `ENABLE_DATA_SCHEDULER` should be true by default with proper `DATA_UPDATE_INTERVAL` (default: 3600s).
- **AWS Configuration**: All AWS credentials must be properly passed for S3 data ingestion.
- **Test Configuration**: `RUN_TESTS_ON_STARTUP` should be set to false in production unless testing is explicitly required.

### 3. Service Orchestration

- **Startup Sequence**: The `setup.sh` script must:
  1. Load environment variables
  2. Initialize directories
  3. Start the data scheduler if enabled
  4. Run initial data ingestion before API startup
  5. Launch the API service

- **Health Monitoring**: Container health checks must verify the API is responsive.

### 3.1 Startup Test Execution

When `RUN_TESTS_ON_STARTUP=true` is set, the container startup sequence is modified:

1. Environment variables are loaded
2. Directories are initialized
3. Tests are executed according to `TEST_TYPE` setting
4. Data ingestion occurs (potentially multiple times during testing)
5. API service is started only after tests complete

**Important considerations:**
- The API will not be available until all tests complete
- Health checks will fail during test execution
- Container may appear unhealthy during this phase
- Set appropriate health check `start_period` to account for test execution time

### 3.2 API Method Compatibility

The API must maintain consistent method names and signatures:

- Public methods should be properly exposed (e.g., `load_stratified_data` vs `_load_stratified_data`)
- Method signatures should remain consistent across versions
- Backward compatibility should be maintained when methods are renamed or refactored
- Consider adding compatibility layers for renamed methods:
  ```python
  # Add compatibility for renamed methods
  def load_stratified_data(self):
      return self._load_stratified_data()
  ```

### 4. Startup Data Loading Considerations

- **Extended Health Check Start Period**: Health checks must account for initial data loading from S3, which can take significant time. Set `start_period` to at least 120-300 seconds.
- **Health Check Retries**: Configure sufficient retries (3-5) to account for data loading delays.
- **Startup Logging**: Ensure comprehensive logging during startup to track data loading progress.
- **False Positive Prevention**: Health checks should not report false positives during initial data loading phase.
- **Graceful Handling**: The application should respond with appropriate status codes during initialization.

### 4.1 Data Processing During Startup

The container startup process includes data processing steps that must be properly handled:

1. **Initial Data Check**: The container checks if data exists and is up-to-date
2. **Data Ingestion**: If data is missing or outdated, it fetches data from S3
3. **Data Stratification**: The complete dataset is stratified according to Chanscope approach
4. **Embedding Generation**: Embeddings are generated for the stratified data in `.npz` format
5. **Initialization Marker**: The `.initialization_in_progress` marker is removed when complete

**Important considerations:**
- This process can take significant time depending on data size
- Health checks should account for this initialization period
- API endpoints should handle requests appropriately during initialization
- Resource contention may occur if tests run during this phase

### 4.2 Resource Contention Handling

To handle resource contention during startup and testing:

1. **File Locking**: Implement proper file locking for shared resources
2. **Graceful Retries**: Add retry logic for operations that may fail due to contention
3. **Cleanup Handling**: Ensure cleanup operations handle "resource busy" errors gracefully
4. **Separate Test Directories**: Consider using separate directories for test data
5. **Initialization State Tracking**: Use atomic operations for initialization state markers

## Testing Environment Requirements

When working with `docker-compose.test.yml`, ensure:

### 1. Test Configuration

- **Test Mode**: `TEST_MODE` environment variable must be set to `true`.
- **Volume Isolation**: Test volumes should be separate from production.
- **Script Mounting**: Test scripts must be mounted as read-write.

### 2. Test Execution

- The test container must:
  1. Execute the complete testing suite via `/app/scripts/run_tests.sh`
  2. Test the data ingestion process with `test_data_ingestion.py`
  3. Test the embedding pipeline with `test_embedding_pipeline.py`
  4. Validate the full Chanscope approach with `test_chanscope_approach.py`

### 3. Resource Allocation

- **Memory Limits**: Minimum 4GB for testing, 8GB for production.
- **CPU Limits**: Appropriate for workload (2 cores for test, 4 for production).

## Replit Deployment Requirements

### 1. Environment Setup

- **Replit Secrets**: Store sensitive environment variables as Replit Secrets:
  - `OPENAI_API_KEY` - Required for model operations
  - `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` - Required for S3 access
  - Additional API keys for alternative model providers (optional)

- **Environment Variables**: Configure in `.replit` or directly via the Replit environment UI:
  - `DATA_RETENTION_DAYS=14` - Controls how far back to fetch data
  - `ENABLE_DATA_SCHEDULER=true` - Enables automatic data updates
  - `DATA_UPDATE_INTERVAL=3600` - Interval for data updates (in seconds)
  - `RUN_TESTS_ON_STARTUP=false` - Normally disabled in production

### 2. Storage Management

- **Replit Database**: Use for small metadata and configuration
- **Replit File System**:
  - `/app/data` - For storing complete and stratified data
  - `/app/data/stratified` - For storing embeddings in `.npz` format and thread ID mappings
  - `/app/logs` - For application logs
  - `/app/temp_files` - For temporary processing files

- **Storage Considerations**:
  - Use `.replit/replit.nix` to install necessary system dependencies
  - Keep embeddings in compressed `.npz` format to conserve space
  - Implement cleanup routines for temporary files
  - Consider S3 for larger datasets that exceed Replit storage limits

### 3. Service Configuration

- **Entry Point**: Configure in `replit.nix` or `.replit`:
  ```
  run = "poetry run uvicorn api.app:app --host 0.0.0.0 --port ${PORT:-8080}"
  ```

- **Poetry Setup**:
  ```
  [tool.poetry]
  name = "chanscope"
  version = "0.1.0"
  description = "Chanscope application for semantic search and inference"
  ```

- **Dependencies**:
  - Ensure all dependencies are in `pyproject.toml`
  - Include development dependencies in a separate group
  - Specify Python version compatible with Replit

### 4. Replit-Specific Optimizations

- **Memory Management**:
  - Implement batch processing for embedding generation
  - Use memory-efficient data structures
  - Implement garbage collection calls at appropriate points

- **Performance Tuning**:
  - Use async processing where possible
  - Implement caching for frequently accessed data
  - Consider smaller sample sizes for stratification

- **API Rate Limiting**:
  - Implement rate limiting for API endpoints
  - Add backoff strategies for external API calls
  - Use circuit breakers for unstable dependencies

### 5. Testing on Replit

- **Test Execution**:
  - Create a dedicated test entry point: `poetry run pytest`
  - Use environment variable `TEST_MODE=true` for test runs
  - Implement test fixtures that clean up after themselves

- **CI Integration**:
  - Configure GitHub Actions to run tests on Replit
  - Use Replit's headless mode for CI/CD pipelines
  - Set up automatic deployment to Replit from CI

## Alignment with Chanscope Approach

All deployment configurations must support the Chanscope approach requirements:

### 1. Data Processing Pipeline

- **Startup Data Flow**:
  - Initial data ingestion from S3
  - Data stratification
  - Embedding generation in `.npz` format with thread ID mapping

### 2. Query Processing Requirements

- **Force Refresh Capability**: Infrastructure must support both refresh patterns:
  - `force_refresh=true`: Re-stratify and regenerate embeddings
  - `force_refresh=false`: Use existing embeddings

### 3. Complete Generative Pipeline Support

- **End-to-End Processing**: Ensure infrastructure supports the complete generative pipeline:
  1. Semantic search to find related content
  2. Chunk processing with LLM for each related string
  3. Final aggregation with summary LLM
  4. Complete response generation with all components

- **Resource Requirements**: Ensure sufficient resources for the complete generative pipeline:
  - Memory for embedding operations
  - API rate limits for model providers
  - Timeout configurations for longer processing sequences

### 4. Embedding Management

- **Storage Format**: Ensure proper handling of `.npz` format embeddings:
  - Compressed numpy arrays for space efficiency
  - Proper thread ID mapping
  - Robust error handling for corruption issues (like CRC-32 errors)

- **Fallback Mechanisms**: Support mock embedding generation when needed:
  - When embedding services are unavailable
  - During testing without API access
  - For development environments

### 5. Testing Framework Support

- The test environment must validate:
  - Initial data load functionality
  - Embedding generation in `.npz` format
  - Incremental processing
  - Force refresh behavior
  - Complete generative pipeline execution

## Security Considerations

- **Non-Root Execution**: Services must run as non-root user (`nobody:nogroup`).
- **Volume Permissions**: Data directories must have appropriate permissions (755/777).
- **Secret Handling**: Sensitive environment variables must be passed through `.env` file or secure secrets management.

## Linter Validation Requirements

Docker Compose files must comply with standard Docker Compose schema:
- Services property must have valid configuration
- Networks must be properly defined if required
- Volumes must be declared appropriately

When encountering linter errors in Docker Compose files, validate:
1. The Docker Compose file version is specified or commented with explanation
2. Top-level keys are valid for the specified version
3. Service, network, and volume configurations follow the official schema

## Deployment Workflow

### 1. Docker Deployment

For standard production deployment:

```bash
# Build and start the application
docker-compose -f deployment/docker-compose.yml build
docker-compose -f deployment/docker-compose.yml up -d
```

### 2. Replit Deployment

For deploying to Replit:

```bash
# Clone the repository
git clone https://github.com/your-org/chanscope.git

# Configure Replit environment
# 1. Add secrets via Replit UI
# 2. Configure environment variables
# 3. Install dependencies with Poetry

# Start the application
poetry run uvicorn api.app:app --host 0.0.0.0 --port ${PORT:-8080}
```

### 3. Testing Before Deployment

For testing before deployment:

```bash
# Run tests first
docker-compose -f deployment/docker-compose.test.yml build
docker-compose -f deployment/docker-compose.test.yml up

# If tests pass, deploy to production
docker-compose -f deployment/docker-compose.yml build
docker-compose -f deployment/docker-compose.yml up -d
```

### 4. Integrated Test and Deploy

For integrated test and deploy workflow:

```bash
# Set RUN_TESTS_ON_STARTUP to true in production environment
docker-compose -f deployment/docker-compose.yml build
docker-compose -f deployment/docker-compose.yml up -d -e RUN_TESTS_ON_STARTUP=true
```

This will:
1. Build the Docker image
2. Start the container
3. Run tests during startup
4. Continue to application deployment if tests pass
5. Log test results for review

### 5. Clean Deployment

For a clean deployment that removes all existing data:

```bash
# Stop and remove all containers and volumes
docker-compose -f deployment/docker-compose.yml down -v

# Rebuild without cache
docker-compose -f deployment/docker-compose.yml build --no-cache

# Start with fresh state
docker-compose -f deployment/docker-compose.yml up -d
```

This ensures a completely fresh environment with no residual data or configurations.

## Monitoring and Maintenance

### 1. Health Checks

The Docker configuration must include health checks to verify:
- API responsiveness
- Data scheduler operation
- Resource utilization

### 1.1 Health Check Configuration for Testing

When tests run during startup, health checks need special configuration:

```yaml
healthcheck:
  test: ["CMD", "curl", "-f", "http://localhost/api/v1/health"]
  interval: 30s
  timeout: 10s
  retries: 5
  start_period: 300s  # Extended to account for test execution
```

The `start_period` should be long enough to allow:
1. Initial data processing
2. Test execution (which may include multiple data processing cycles)
3. API startup

### 2. Logging

Proper logging configuration must be in place:
- Application logs in `/app/logs`
- Scheduler logs in `scheduler.log`
- Container logs accessible via `docker-compose logs`
- Utility logs should be consolidated in the `/app/logs` directory, not in the application root
- All log files should use the centralized logging configuration from `config/logging_config.py`

### 3. Backup and Recovery

Data volumes should be configured for backup:
- Regular snapshots of `/app/data`
- Backup of configuration in `.env`
- Documentation of recovery procedures

## Troubleshooting

### 1. Container Startup Issues

If the container fails to start:
1. Check logs: `docker-compose logs`
2. Verify environment variables
3. Check volume permissions
4. Validate network configuration

### 2. Test Failures

If tests fail during startup:
1. Check test logs in `test_results/`
2. Verify AWS credentials
3. Check for network connectivity issues
4. Validate data access permissions

### 3. Performance Issues

If the application performs poorly:
1. Check resource allocation in docker-compose.yml
2. Monitor CPU and memory usage
3. Verify data volume performance
4. Adjust worker count and resource limits

### 4. Data Loading Delays

If health checks fail during initial startup:
1. Verify S3 connectivity and credentials
2. Check logs for data loading progress
3. Consider increasing health check `start_period` and `retries`
4. Ensure the application responds with appropriate status during initialization
5. Monitor data processing logs to track progress

### 5. API Method Compatibility Issues

If you encounter errors like `'DataOperations' object has no attribute 'load_stratified_data'`:

1. Check for method name changes in recent code updates
2. Verify that all required methods are properly exposed in the API
3. Add compatibility layers for renamed methods:
   ```python
   # In data_ops.py
   def load_stratified_data(self):
       """Compatibility method for backward compatibility"""
       return self._load_stratified_data()
   ```
4. Update tests to match the current API interface
5. Consider adding interface validation tests to catch these issues early

### 6. Resource Contention Issues

If you encounter errors like `[Errno 16] Device or resource busy: '/app/data'`:

1. This typically occurs when tests run during container startup
2. The error indicates that multiple processes are trying to access the same files
3. Set `ABORT_ON_TEST_FAILURE=false` to continue despite these errors
4. Consider implementing file locking mechanisms in the application
5. Use separate data directories for tests to avoid contention
6. Add retry logic for operations that may fail due to resource contention

### 7. Embedding Format Issues

If you encounter errors like `Bad CRC-32` when loading embeddings:

1. This indicates corruption in the embedding file format
2. Verify that embeddings are being saved in `.npz` format, not `.npy`
3. Check for proper file locking during embedding save operations
4. Ensure proper atomic write operations for embedding files
5. Validate that the thread ID mapping matches the embedding indices
6. Use the backup mechanism to restore from previous versions

### 8. Generative Pipeline Issues

If the generative pipeline produces incomplete results:

1. Verify that all stages of the pipeline are being executed:
   - Semantic search for related content
   - Chunk processing for each related string
   - Final aggregation and summarization
2. Check for timeout issues during longer processing sequences
3. Ensure sufficient memory is available for embedding operations
4. Validate API rate limits for model providers
5. Check for proper error handling in each stage of the pipeline

## Example Configurations

### Production Docker Configuration

```yaml
# yaml-language-server: $schema=https://raw.githubusercontent.com/compose-spec/compose-spec/master/schema/compose-spec.json
# The version attribute is now auto-detected and the explicit declaration is obsolete

services:
  app:
    build:
      context: ..
      dockerfile: deployment/Dockerfile
    volumes:
      # Configuration files
      - type: bind
        source: ../.env
        target: /app/.env
        read_only: true
      # Application code
      - type: bind
        source: ../api
        target: /app/api
        read_only: true
      - type: bind
        source: ../config
        target: /app/config
        read_only: true
      - type: bind
        source: ../knowledge_agents
        target: /app/knowledge_agents
        read_only: true
      - type: volume
        source: app_data
        target: /app/data
      # Persistent volumes
      - type: volume
        source: poetry_cache
        target: /app/.cache/pypoetry
      - type: volume
        source: app_logs
        target: /app/logs
      - type: volume
        source: app_temp
        target: /app/temp_files
    environment:
      # Docker environment settings
      - DOCKER_ENV=true
      - PYTHONPATH=/app
      # Data management settings
      - DATA_RETENTION_DAYS=14
      - ENABLE_DATA_SCHEDULER=true
      - DATA_UPDATE_INTERVAL=3600
      # Test execution control
      - RUN_TESTS_ON_STARTUP=false
      - TEST_MODE=false
```

### Test Docker Configuration

```yaml
# yaml-language-server: $schema=https://raw.githubusercontent.com/compose-spec/compose-spec/master/schema/compose-spec.json
# The version attribute is now auto-detected and the explicit declaration is obsolete

services:
  chanscope-test:
    build:
      context: ..
      dockerfile: deployment/Dockerfile
    volumes:
      # Configuration files
      - type: bind
        source: ../.env.test
        target: /app/.env
        read_only: true
      # Test files with read-write for scripts and results
      - type: bind
        source: ../scripts
        target: /app/scripts
      - type: bind
        source: ../tests
        target: /app/tests
        read_only: true
      - type: bind
        source: ../test_results
        target: /app/test_results
      # Isolated test volumes
      - type: volume
        source: test_data
        target: /app/data
      - type: volume
        source: test_logs
        target: /app/logs
    environment:
      # Test environment settings
      - TEST_MODE=true
      # Test execution control
      - RUN_TESTS_ON_STARTUP=true
      - TEST_TYPE=all
    command: ["bash", "/app/scripts/run_tests.sh", "--all"]
```

### Replit Configuration

```toml
# .replit
run = "poetry run uvicorn api.app:app --host 0.0.0.0 --port ${PORT:-8080}"
language = "python3"

[env]
DATA_RETENTION_DAYS = "14"
ENABLE_DATA_SCHEDULER = "true"
DATA_UPDATE_INTERVAL = "3600"
RUN_TESTS_ON_STARTUP = "false"
TEST_MODE = "false"
PYTHONPATH = "${REPL_HOME}"

[nix]
channel = "stable-22_11"

[packager]
language = "python3"

[packager.features]
packageSearch = true
guessImports = true

[unitTest]
language = "python3"

[deployment]
run = ["sh", "-c", "poetry run uvicorn api.app:app --host 0.0.0.0 --port ${PORT:-8080}"]
deploymentTarget = "cloudrun"
```

## Windows-Specific Considerations

When deploying in Windows environments:

1. Path formatting:
   - Use forward slashes in Docker Compose files
   - Convert Windows paths when mounting volumes

2. Line endings:
   - Ensure all scripts have LF line endings
   - Use `.gitattributes` to enforce proper line endings

3. Docker Desktop settings:
   - Allocate sufficient resources in Docker Desktop
   - Enable WSL2 backend for better performance
