---
description: This rule file provides guidelines for Docker deployment, ensuring volumes, environment variables, and service orchestration align with the Chanscope approach's data processing requirement
globs: */docker-compose*.yml", */Docker*, */deployment/**/*.sh
alwaysApply: false
---
# Chanscope Deployment Rules

This file provides guidelines for Chanscope deployment with Docker, ensuring alignment between infrastructure components and the core Chanscope approach.

## Description

These rules define the operational requirements for containerized deployment of the Chanscope application, ensuring that Docker configurations properly support the data ingestion, processing, and query pipeline defined in the Chanscope approach.

## File Patterns

```
**/docker-compose*.yml
**/Docker*
**/deployment/**/*.sh
```

## Production Deployment Requirements

When working with `docker-compose.yml` and `Dockerfile`, ensure the following requirements are met:

### 1. Volume Configuration

- **Data Persistence**: The Docker setup must maintain persistent volumes for:
  - `/app/data` - For storing complete and stratified data
  - `/app/logs` - For application logs
  - `/app/temp_files` - For temporary processing files

- **Volume Ownership**: Volumes must be accessible to the non-root user (`nobody:nogroup`) running the container.

### 2. Environment Configuration

- **Data Retention**: `DATA_RETENTION_DAYS` must be set appropriately (default: 30).
- **Data Scheduler**: `ENABLE_DATA_SCHEDULER` should be true by default with proper `DATA_UPDATE_INTERVAL` (default: 3600s).
- **AWS Configuration**: All AWS credentials must be properly passed for S3 data ingestion.
- **Test Configuration**: `RUN_TESTS_ON_STARTUP` should be set to false in production unless testing is explicitly required.

### 3. Service Orchestration

- **Startup Sequence**: The `setup.sh` script must:
  1. Load environment variables
  2. Initialize directories
  3. Start the data scheduler if enabled
  4. Run initial data ingestion before API startup
  5. Launch the API service

- **Health Monitoring**: Container health checks must verify the API is responsive.

### 3.1 Startup Test Execution

When `RUN_TESTS_ON_STARTUP=true` is set, the container startup sequence is modified:

1. Environment variables are loaded
2. Directories are initialized
3. Tests are executed according to `TEST_TYPE` setting
4. Data ingestion occurs (potentially multiple times during testing)
5. API service is started only after tests complete

**Important considerations:**
- The API will not be available until all tests complete
- Health checks will fail during test execution
- Container may appear unhealthy during this phase
- Set appropriate health check `start_period` to account for test execution time

### 3.2 API Method Compatibility

The API must maintain consistent method names and signatures:

- Public methods should be properly exposed (e.g., `load_stratified_data` vs `_load_stratified_data`)
- Method signatures should remain consistent across versions
- Backward compatibility should be maintained when methods are renamed or refactored
- Consider adding compatibility layers for renamed methods:
  ```python
  # Add compatibility for renamed methods
  def load_stratified_data(self):
      return self._load_stratified_data()
  ```

### 4. Startup Data Loading Considerations

- **Extended Health Check Start Period**: Health checks must account for initial data loading from S3, which can take significant time. Set `start_period` to at least 60-120 seconds.
- **Health Check Retries**: Configure sufficient retries (3-5) to account for data loading delays.
- **Startup Logging**: Ensure comprehensive logging during startup to track data loading progress.
- **False Positive Prevention**: Health checks should not report false positives during initial data loading phase.
- **Graceful Handling**: The application should respond with appropriate status codes during initialization.

### 4.1 Data Processing During Startup

The container startup process includes data processing steps that must be properly handled:

1. **Initial Data Check**: The container checks if data exists and is up-to-date
2. **Data Ingestion**: If data is missing or outdated, it fetches data from S3
3. **Data Stratification**: The complete dataset is stratified according to Chanscope approach
4. **Embedding Generation**: Embeddings are generated for the stratified data
5. **Initialization Marker**: The `.initialization_in_progress` marker is removed when complete

**Important considerations:**
- This process can take significant time depending on data size
- Health checks should account for this initialization period
- API endpoints should handle requests appropriately during initialization
- Resource contention may occur if tests run during this phase

### 4.2 Resource Contention Handling

To handle resource contention during startup and testing:

1. **File Locking**: Implement proper file locking for shared resources
2. **Graceful Retries**: Add retry logic for operations that may fail due to contention
3. **Cleanup Handling**: Ensure cleanup operations handle "resource busy" errors gracefully
4. **Separate Test Directories**: Consider using separate directories for test data
5. **Initialization State Tracking**: Use atomic operations for initialization state markers

## Testing Environment Requirements

When working with `docker-compose.test.yml`, ensure:

### 1. Test Configuration

- **Test Mode**: `TEST_MODE` environment variable must be set to `true`.
- **Volume Isolation**: Test volumes should be separate from production.
- **Script Mounting**: Test scripts must be mounted as read-write.

### 2. Test Execution

- The test container must:
  1. Execute the complete testing suite via `/app/scripts/run_tests.sh`
  2. Test the data ingestion process with `test_data_ingestion.py`
  3. Test the embedding pipeline with `test_embedding_pipeline.py`
  4. Validate the full Chanscope approach with `test_chanscope_approach.py`

### 3. Resource Allocation

- **Memory Limits**: Minimum 4GB for testing, 8GB for production.
- **CPU Limits**: Appropriate for workload (2 cores for test, 4 for production).

## Alignment with Chanscope Approach

All Docker configurations must support the Chanscope approach requirements:

### 1. Data Processing Pipeline

- **Startup Data Flow**: Containers must support:
  - Initial data ingestion from S3
  - Data stratification
  - Embedding generation

### 2. Query Processing Requirements

- **Force Refresh Capability**: Infrastructure must support both refresh patterns:
  - `force_refresh=true`: Re-stratify and regenerate embeddings
  - `force_refresh=false`: Use existing embeddings

### 3. Testing Framework Support

- The test environment must validate:
  - Initial data load functionality
  - Embedding generation
  - Incremental processing
  - Force refresh behavior

## Security Considerations

- **Non-Root Execution**: Services must run as non-root user (`nobody:nogroup`).
- **Volume Permissions**: Data directories must have appropriate permissions (755/777).
- **Secret Handling**: Sensitive environment variables must be passed through `.env` file or secure secrets management.

## Linter Validation Requirements

Docker Compose files must comply with standard Docker Compose schema:
- Services property must have valid configuration
- Networks must be properly defined if required
- Volumes must be declared appropriately

When encountering linter errors in Docker Compose files, validate:
1. The Docker Compose file version is specified
2. Top-level keys are valid for the specified version
3. Service, network, and volume configurations follow the official schema

## Deployment Workflow

### 1. Standard Deployment

For standard production deployment:

```bash
# Build and start the application
docker-compose -f deployment/docker-compose.yml build
docker-compose -f deployment/docker-compose.yml up -d
```

### 2. Testing Before Deployment

For testing before deployment:

```bash
# Run tests first
docker-compose -f deployment/docker-compose.test.yml build
docker-compose -f deployment/docker-compose.test.yml up

# If tests pass, deploy to production
docker-compose -f deployment/docker-compose.yml build
docker-compose -f deployment/docker-compose.yml up -d
```

### 3. Integrated Test and Deploy

For integrated test and deploy workflow:

```bash
# Set RUN_TESTS_ON_STARTUP to true in production environment
docker-compose -f deployment/docker-compose.yml build
docker-compose -f deployment/docker-compose.yml up -d -e RUN_TESTS_ON_STARTUP=true
```

This will:
1. Build the Docker image
2. Start the container
3. Run tests during startup
4. Continue to application deployment if tests pass
5. Log test results for review

### 4. Clean Deployment

For a clean deployment that removes all existing data:

```bash
# Stop and remove all containers and volumes
docker-compose -f deployment/docker-compose.yml down -v

# Rebuild without cache
docker-compose -f deployment/docker-compose.yml build --no-cache

# Start with fresh state
docker-compose -f deployment/docker-compose.yml up -d
```

This ensures a completely fresh environment with no residual data or configurations.

## Monitoring and Maintenance

### 1. Health Checks

The Docker configuration must include health checks to verify:
- API responsiveness
- Data scheduler operation
- Resource utilization

### 1.1 Health Check Configuration for Testing

When tests run during startup, health checks need special configuration:

```yaml
healthcheck:
  test: ["CMD", "curl", "-f", "http://localhost/api/v1/health"]
  interval: 30s
  timeout: 10s
  retries: 5
  start_period: 300s  # Extended to account for test execution
```

The `start_period` should be long enough to allow:
1. Initial data processing
2. Test execution (which may include multiple data processing cycles)
3. API startup

### 2. Logging

Proper logging configuration must be in place:
- Application logs in `/app/logs`
- Scheduler logs in `/app/data/logs/scheduler.log`
- Container logs accessible via `docker-compose logs`
- Utility logs should be consolidated in the `/app/logs` directory, not in the application root
- All log files should use the centralized logging configuration from `config/logging_config.py`

### 3. Backup and Recovery

Data volumes should be configured for backup:
- Regular snapshots of `/app/data`
- Backup of configuration in `.env`
- Documentation of recovery procedures

## Troubleshooting

### 1. Container Startup Issues

If the container fails to start:
1. Check logs: `docker-compose logs`
2. Verify environment variables
3. Check volume permissions
4. Validate network configuration

### 2. Test Failures

If tests fail during startup:
1. Check test logs in `test_results/`
2. Verify AWS credentials
3. Check for network connectivity issues
4. Validate data access permissions

### 3. Performance Issues

If the application performs poorly:
1. Check resource allocation in docker-compose.yml
2. Monitor CPU and memory usage
3. Verify data volume performance
4. Adjust worker count and resource limits

### 4. Data Loading Delays

If health checks fail during initial startup:
1. Verify S3 connectivity and credentials
2. Check logs for data loading progress
3. Consider increasing health check `start_period` and `retries`
4. Ensure the application responds with appropriate status during initialization
5. Monitor data processing logs to track progress

### 5. API Method Compatibility Issues

If you encounter errors like `'DataOperations' object has no attribute 'load_stratified_data'`:

1. Check for method name changes in recent code updates
2. Verify that all required methods are properly exposed in the API
3. Add compatibility layers for renamed methods:
   ```python
   # In data_ops.py
   def load_stratified_data(self):
       """Compatibility method for backward compatibility"""
       return self._load_stratified_data()
   ```
4. Update tests to match the current API interface
5. Consider adding interface validation tests to catch these issues early

### 6. Resource Contention Issues

If you encounter errors like `[Errno 16] Device or resource busy: '/app/data'`:

1. This typically occurs when tests run during container startup
2. The error indicates that multiple processes are trying to access the same files
3. Set `ABORT_ON_TEST_FAILURE=false` to continue despite these errors
4. Consider implementing file locking mechanisms in the application
5. Use separate data directories for tests to avoid contention
6. Add retry logic for operations that may fail due to resource contention

## Example Configurations

### Production Configuration

```yaml
version: '3.8'
services:
  app:
    build:
      context: ..
      dockerfile: deployment/Dockerfile
    volumes:
      - type: volume
        source: app_data
        target: /app/data
      - type: volume
        source: app_logs
        target: /app/logs
    environment:
      - DATA_RETENTION_DAYS=30
      - ENABLE_DATA_SCHEDULER=true
      - RUN_TESTS_ON_STARTUP=false
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
```

### Test Configuration

```yaml
version: '3.8'
services:
  chanscope-test:
    build:
      context: ..
      dockerfile: deployment/Dockerfile
    volumes:
      - type: volume
        source: test_data
        target: /app/data
    environment:
      - TEST_MODE=true
      - RUN_TESTS_ON_STARTUP=true
    command: ["bash", "/app/scripts/run_tests.sh", "--all"]
```

### Integrated Test and Deploy

```yaml
version: '3.8'
services:
  app:
    build:
      context: ..
      dockerfile: deployment/Dockerfile
    environment:
      - RUN_TESTS_ON_STARTUP=true
      - TEST_TYPE=all
      - AUTO_CHECK_DATA=true
      - ABORT_ON_TEST_FAILURE=false
    healthcheck:
      start_period: 300s  # Extended to account for test execution
```

### Clean Deployment Configuration

```yaml
version: '3.8'
services:
  app:
    build:
      context: ..
      dockerfile: deployment/Dockerfile
    volumes:
      - type: volume
        source: app_data
        target: /app/data
        # Use 'nocopy' option to ensure a clean volume
        volume:
          nocopy: true
    environment:
      - FORCE_DATA_REFRESH=true  # Force data refresh on startup
```

## Windows-Specific Considerations

When deploying in Windows environments:

1. Path formatting:
   - Use forward slashes in Docker Compose files
   - Convert Windows paths when mounting volumes

2. Line endings:
   - Ensure all scripts have LF line endings
   - Use `.gitattributes` to enforce proper line endings

3. Docker Desktop settings:
   - Allocate sufficient resources in Docker Desktop
   - Enable WSL2 backend for better performance