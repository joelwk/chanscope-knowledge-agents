---
description: This document outlines the guidelines for testing the Chanscope application, ensuring that test implementation aligns with the core Chanscope approach. It covers the data processing pipeline, query behavior, deployment processes, and more.
globs: */tests/**/*.py,*/scripts/run_*tests.sh,*/scripts/validate_chanscope_approach.py
alwaysApply: false
---
# Chanscope Testing Rules

## Test Suite Organization

### 1. Core Test Categories

Tests must be organized to validate all key components of the Chanscope approach:

- **Data Ingestion Tests:**  
  Verify S3 data fetching, retention logic, and initial data load (e.g., `test_data_ingestion.py`).

- **Data Stratification Tests:**  
  Validate sampling algorithms and data representation accuracy.

- **Embedding Generation Tests:**  
  Ensure proper embedding creation, storage, and consistency (e.g., `test_embedding_pipeline.py`).

- **Query Processing Tests:**  
  Validate both forced refresh and standard modes:
  - **Force Refresh Mode:** Ensure data freshness, re-stratification, and regeneration of embeddings.
  - **Standard Mode:** Verify that existing embeddings are used and fallback logic is applied if necessary.

- **API Availability Tests:**  
  Verify API endpoints remain responsive even during data initialization, with appropriate status indicators.

- **End-to-End Tests:**  
  Validate the complete Chanscope approach implementation (e.g., `test_chanscope_approach.py`).

- **Component Integration Tests:**  
  Validate integration between data ingestion, stratification, embedding generation, search, chunk retrieval, and final summarization.

- **Startup Data Loading Tests:**  
  Verify health check behavior, initialization state detection, and progress monitoring during data loading delays.

### 2. Test Script Requirements

All test scripts must:

- Be executable in isolated environments (Docker, local, or Replit).
- Include proper setup and teardown routines to avoid interference.
- Generate clear, structured logs and output.
- Return appropriate exit codes for CI/CD integration.

### 3. Cross-Platform Compatibility

Test scripts must be designed for use across multiple platforms:

- **Linux/macOS:**  
  Use standard bash scripts (e.g., `*.sh`).

- **Windows:**  
  Provide PowerShell equivalents or ensure compatibility by:
  - Using backslashes for file paths.
  - Replacing bash-specific commands with PowerShell equivalents.
  - Handling line endings (CRLF vs LF) properly.
  - Adapting Docker commands to Windows path conventions (e.g., `C:/path` vs `/c/path`).

---

# Chanscope Testing Rules

This document outlines the guidelines for testing the Chanscope application, ensuring that test implementation aligns with the core Chanscope approach. It covers the data processing pipeline, query behavior, deployment processes, and more.

---

## Test Suite Organization

### 1. Core Test Categories

Tests must be organized to validate all key components of the Chanscope approach:

- **Data Ingestion Tests:**  
  Verify S3 data fetching, retention logic, and initial data load (e.g., `test_data_ingestion.py`).

- **Data Stratification Tests:**  
  Validate sampling algorithms and data representation accuracy.

- **Embedding Generation Tests:**  
  Ensure proper embedding creation, storage, and consistency (e.g., `test_embedding_pipeline.py`).

- **Query Processing Tests:**  
  Validate both forced refresh and standard modes:
  - **Force Refresh Mode:** Ensure data freshness, re-stratification, and regeneration of embeddings.
  - **Standard Mode:** Verify that existing embeddings are used and fallback logic is applied if necessary.

- **API Availability Tests:**  
  Verify API endpoints remain responsive even during data initialization, with appropriate status indicators.

- **End-to-End Tests:**  
  Validate the complete Chanscope approach implementation (e.g., `test_chanscope_approach.py`).

- **Component Integration Tests:**  
  Validate integration between data ingestion, stratification, embedding generation, search, chunk retrieval, and final summarization.

- **Startup Data Loading Tests:**  
  Verify health check behavior, initialization state detection, and progress monitoring during data loading delays.

### 2. Test Script Requirements

All test scripts must:

- Be executable in isolated environments (Docker, local, or Replit).
- Include proper setup and teardown routines to avoid interference.
- Generate clear, structured logs and output.
- Return appropriate exit codes for CI/CD integration.

### 3. Cross-Platform Compatibility

Test scripts must be designed for use across multiple platforms:

- **Linux/macOS:**  
  Use standard bash scripts (e.g., `*.sh`).

- **Windows:**  
  Provide PowerShell equivalents or ensure compatibility by:
  - Using backslashes for file paths.
  - Replacing bash-specific commands with PowerShell equivalents.
  - Handling line endings (CRLF vs LF) properly.
  - Adapting Docker commands to Windows path conventions (e.g., `C:/path` vs `/c/path`).

---

## Updated Test Execution Framework

### 1. Standard Test Execution

The testing framework provides a unified approach across all environments. Use the `run_tests.sh` script as the main entry point:

```bash
# Run all tests (auto-detects environment)
scripts/run_tests.sh

# Run specific test categories
scripts/run_tests.sh --data-ingestion
scripts/run_tests.sh --embedding
scripts/run_tests.sh --endpoints
scripts/run_tests.sh --chanscope-approach

# Specify environment explicitly
scripts/run_tests.sh --env=local
scripts/run_tests.sh --env=docker
scripts/run_tests.sh --env=replit

# Display help and options
scripts/run_tests.sh --help
```

### 2. Environment-Specific Scripts

The framework includes dedicated scripts for each environment:

- **`local_tests.sh`**: Handles local environment setup and test execution
- **`docker_tests.sh`**: Manages Docker-specific configuration and container execution
- **`replit_tests.sh`**: Addresses Replit environment constraints and configuration

These scripts are called by `run_tests.sh` based on environment detection or explicit specification.

### 3. Docker-Based Testing

For containerized testing, you can either:

1. Use the main test runner with Docker environment:
   ```bash
   scripts/run_tests.sh --env=docker
   ```

2. Or use the dedicated Docker Compose configuration directly:
   ```bash
   # From project root
   docker-compose -f deployment/docker-compose.test.yml build
   docker-compose -f deployment/docker-compose.test.yml up
   ```

### 4. Test and Deploy Workflow

The `test_and_deploy.sh` script provides an integrated workflow:

```bash
# Run tests and deploy as separate steps
scripts/test_and_deploy.sh

# Specify environment for testing
scripts/test_and_deploy.sh --env=docker

# Use integrated test and deploy workflow
scripts/test_and_deploy.sh --integrated
```

### 5. Test Configuration Settings

Test configuration is centralized in `.env.test` with environment-specific sections:

```bash
# Shared configuration (all environments)
TEST_MODE=true
USE_MOCK_DATA=true
USE_MOCK_EMBEDDINGS=true
AUTO_CHECK_DATA=true
TEST_TYPE=all

# Environment-specific configuration
[local]
API_WORKERS=2
PROCESSING_CHUNK_SIZE=2500
# ...other local settings

[docker]
API_WORKERS=4
PROCESSING_CHUNK_SIZE=5000
# ...other Docker settings

[replit]
API_WORKERS=1
PROCESSING_CHUNK_SIZE=1000
# ...other Replit settings
```

---

## Validation Against Chanscope Approach

### 1. Initial Data Load Validation

Tests must ensure that on startup:

- Data is correctly ingested from S3.
- Data is properly stratified.
- Embeddings are generated as expected.

Example (from `test_chanscope_approach.py`):

```python
@pytest.mark.asyncio
async def test_initial_data_load(chanscope_test_config, results_collector):
    """Test initial data load behavior (force_refresh=true, skip_embeddings=true)"""
    # Arrange: Prepare clean environment 
    # Act: Initialize data operations with force_refresh=true
    # Assert: Verify that complete_data.csv and stratified data exist
```

### 2. Query Behavior Validation

Tests must cover both query modes:

#### 2.1 Force Refresh Mode

- Verify that when `force_refresh=true`:
  - Data freshness is checked.
  - A new stratified sample is generated.
  - New embeddings are created.
  - Queries yield appropriate chunks and summaries.

#### 2.2 Standard Mode

- Verify that when `force_refresh=false`:
  - Existing embeddings are used (without regeneration).
  - Query processing returns expected results.
  - If data is missing, fallback to force refresh behavior.

### 3. Component Integration Tests

Tests validate the integration between:

- Data ingestion and stratification.
- Stratification and embedding generation.
- Embedding search and chunk retrieval.
- Chunk processing and final summarization.

### 4. Startup Data Loading Tests

Tests address:

- **Health Check Testing:** Validate health checks during initialization.
- **Initialization State Detection:** Confirm the application reports its initialization state correctly.
- **API Availability:** Verify API remains responsive even during initialization.
- **Background Processing:** Confirm data processing continues in the background without blocking API.
- **Progress Monitoring:** Ensure that data loading progress is logged and tracked.

Example (from `test_endpoints.py`):

```python
def test_health_during_initialization():
    # Arrange: Start application with mock S3 data and AUTO_CHECK_DATA=false
    # Act: Query health endpoint immediately after startup
    # Assert: Verify health endpoint returns appropriate status during initialization
```

### 5. AUTO_CHECK_DATA Setting Tests

Tests verify the behavior with different AUTO_CHECK_DATA settings to ensure proper handling of both automatic and manual data checking modes.

---

## Deployment Test Requirements

### 1. Docker Test Environment

Tests validate Docker-specific elements:

- **Volume Mounting and Permissions:**  
  - Use Docker volumes (not bind mounts) for data directories.
  - Set proper permissions (e.g., chmod -R 777) for directories such as `/app/data`, `/app/data/stratified`, `/app/data/shared`, `/app/logs`, and `/app/temp_files`.

- **Environment Variable Handling:**  
  - Ensure environment variables are properly set (e.g., `TEST_MODE=true`).

- **Service Startup and Orchestration:**  
  - Validate that the docker-compose configuration overrides the default command to run tests.
  - Disable health check and auto-restart during tests to prevent interference.

### 2. CI/CD Integration

Test scripts are compatible with automated CI/CD pipelines, including:

- Clear indicators of success/failure.
- Detailed error reporting.
- Standardized output formats.
- Appropriate timeouts for long-running tests.

Example CI snippet:

```yaml
test:
  stage: test
  script:
    - docker-compose -f deployment/docker-compose.test.yml build
    - docker-compose -f deployment/docker-compose.test.yml run --rm chanscope-test
  artifacts:
    paths:
      - test_results/
```

### 3. Multi-Environment Testing

The framework provides support for multi-environment testing:

- **Environment Detection**: Automatically identifies the current environment
- **Environment-Specific Scripts**: Dedicated scripts for each environment
- **Unified Command Interface**: Consistent commands across all environments
- **Centralized Configuration**: Environment-specific settings in `.env.test`

---

## Environment Setup for Testing

### 1. Local Environment Setup

```bash
# Clone the repository
git clone https://github.com/your-org/chanscope.git
cd chanscope

# Create and populate .env file with required variables (AWS, OpenAI, etc.)
cp .env.template .env

# Make scripts executable
chmod +x scripts/*.sh deployment/*.sh

# Run tests
scripts/run_tests.sh --env=local
```

### 2. Docker Environment Setup

```bash
# Clone the repository
git clone https://github.com/your-org/chanscope.git
cd chanscope

# Create and populate .env file
cp .env.template .env

# Run tests in Docker
scripts/run_tests.sh --env=docker
```

### 3. Replit Environment Setup

```bash
# Set environment variables in Replit Secrets:
# OPENAI_API_KEY, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION, S3_BUCKET

# Run tests
scripts/run_tests.sh --env=replit
```

---

## Test Data Management

### 1. Test Fixtures

- Use consistent test data across suites, covering both typical and edge cases.
- Store test data in version control when practical.

### 2. Mock Services

- **Mock External Dependencies:**  
  Use mocks for services such as S3 and the OpenAI API to isolate tests.
- **Document Mock Behavior:**  
  Ensure that the behavior of mocks is well-documented for maintainability.

### 3. Mock Embeddings

- Set `USE_MOCK_EMBEDDINGS=true` to bypass external embedding models.
- Ensure mock embeddings have consistent dimensions (e.g., 3072) for reproducibility.
- Document in test results how mocks are used versus real embeddings.

```bash
# Enable mock embeddings for testing
scripts/run_tests.sh --no-mock-embeddings
```

---

## Performance Testing

Include tests to validate performance criteria:

- **Data Refresh Performance:**  
  Measure time required to refresh data under various conditions.
- **Query Response Time:**  
  Validate that query response times meet established thresholds (with and without caching).
- **Resource Utilization:**  
  Monitor CPU and memory usage during intensive operations.
- **API Responsiveness During Data Processing**
  Measure API response times during background data processing.

### Performance Metrics Collection

Tests should capture:

- Time for data stratification.
- Time for embedding generation.
- Query response times.
- Memory usage during peak operations.
- API availability during background processing.

```bash
# Enable performance testing mode
export PERFORMANCE_TEST=true
export ITERATIONS=10

# Run performance tests
scripts/run_tests.sh --chanscope-approach
```

---

## Security and Error Handling Tests

- **Authentication Tests:**  
  Validate API security and access controls.
- **Error Handling Tests:**  
  Verify that failures occur gracefully and resources are cleaned up.
- **Resource Cleanup:**  
  Ensure that temporary files and connections are properly disposed of after tests.

### Common Validation Issues

Tests should verify handling of:

- Short text articles below the minimum threshold.
- Articles with low character diversity.
- Invalid or corrupted data.
- Connection failures to external services.
- Partially processed data scenarios.

---

## Logging and Monitoring

### 1. Centralized Logging

- All tests must use the centralized logging configuration (`config/logging_config.py`).
- Avoid direct calls to `logging.basicConfig()` within test code.
- Ensure log files are written to `/app/logs` and that utility logs (e.g., `utility_func.log`) are correctly redirected.

### 2. Log Consolidation

Tests should verify that:

- Logs are written to the correct directory.
- No logs appear in the application root.
- Log rotation is configured properly.
- Log levels are enforced as per configuration.

Example (test verification):

```python
def test_log_configuration():
    # Arrange: Set up logging environment with controlled log level
    # Act: Generate logs at various levels
    # Assert: Confirm logs are in /app/logs with correct formatting and rotation
```

---

## Troubleshooting Common Issues

### 1. AWS Credential Issues

If tests fail due to AWS credentials:

1. Confirm that the `.env` file has correct credentials.
2. Check that environment variables are properly set in the shell or Docker.
3. Verify that S3_BUCKET is accessible.
4. Consider using mock data by setting `export USE_MOCK_DATA=true`.

### 2. Test Output and Logs

For diagnosing issues:

```bash
# View test logs (environment-specific)
cat test_results/chanscope_tests_local_*.log
cat test_results/chanscope_tests_docker_*.log
cat test_results/chanscope_tests_replit_*.log

# View application logs
cat logs/app.log

# View scheduler logs
cat scheduler.log
```

### 3. Docker Volume Issues

If Docker volumes encounter permission issues:

1. Use the test Docker configuration that correctly sets up volumes.
2. Verify volume mappings in docker-compose files.
3. Ensure the Docker user (or 'nobody') has appropriate permissions.

### 4. Windows-Specific Issues

- Adapt file paths (backslashes vs forward slashes).
- Convert CRLF to LF for scripts using `.gitattributes`.
- Adjust script execution policies if necessary (e.g., using PowerShell).
- Ensure you're running docker-compose commands from the deployment directory or specifying the full path.

### 5. "No Configuration File Found" Error

If you encounter "no configuration file provided: not found" error:
1. Make sure you're running docker-compose from the project root:
   ```
   docker-compose -f deployment/docker-compose.yml logs -f
   ```
2. Or specify the full path to the docker-compose file:
   ```
   docker-compose -f deployment/docker-compose.yml logs -f
   ```

### 6. Container Restart Loops

If container keeps restarting:
1. Set `AUTO_CHECK_DATA=false` in the environment configuration
2. Check logs for specific error messages
3. Verify S3 connectivity and credentials
4. Ensure volume permissions are correct

### 7. API Unavailability During Data Processing

If the API is unavailable during data processing:
1. Verify `AUTO_CHECK_DATA=false` is set
2. Check the health check configuration (start_period, interval, retries)
3. Review logs for initialization issues
4. Consider increasing the container's resource allocation

---

## Recommended Testing Workflow

### 1. Standard Workflow (Recommended) ✅

Run tests in isolation using the dedicated testing framework:

```bash
# Auto-detect environment
scripts/run_tests.sh

# Specify environment explicitly
scripts/run_tests.sh --env=docker
```

- Ensures isolated test data and resources
- Optimized environment variables and resource allocation for testing

### 2. Production Deployment (Tests Disabled) ✅

Deploy your application without running tests at startup:

```bash
docker-compose -f deployment/docker-compose.yml build
docker-compose -f deployment/docker-compose.yml up -d
```

- Ensures stable, predictable startup behavior
- Avoids resource contention and data integrity risks

### 3. Integrated Test and Deploy (Advanced, Optional) ⚠️

Running tests as part of deployment is possible but should be reserved for staging environments:

```bash
scripts/test_and_deploy.sh --integrated
```

- Requires careful handling of resource contention and data isolation
- Not recommended for live production environments
