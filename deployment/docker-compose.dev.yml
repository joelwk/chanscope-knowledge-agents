version: '3.8'

services:
  api:
    build:
      context: ..
      dockerfile: deployment/Dockerfile
      target: development
    volumes:
      - ../api:/app/api
      - ../knowledge_agents:/app/knowledge_agents
      - ../config:/app/config
      - ../data:/app/data
      - ../logs:/app/logs
      - ../pyproject.toml:/app/pyproject.toml
      - ../poetry.lock:/app/poetry.lock
      - ../.env:/app/.env
    environment:
      # Service Type
      - SERVICE_TYPE=api
      - DOCKER_ENV=true
      
      # Port Configuration
      - PORT=5000
      - HOST=0.0.0.0
      
      # Development Settings
      - QUART_APP=api.app
      - QUART_ENV=development
      - PYTHONPATH=/app
      - LOG_LEVEL=${LOG_LEVEL:-DEBUG}

      # API Keys
      - OPENAI_API_KEY=${OPENAI_API_KEY}  # Required, no default
      - GROK_API_KEY=${GROK_API_KEY}      # Optional
      - VENICE_API_KEY=${VENICE_API_KEY}   # Optional

      # Model Settings
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4o}
      - OPENAI_EMBEDDING_MODEL=${OPENAI_EMBEDDING_MODEL:-text-embedding-3-large}
      - GROK_MODEL=${GROK_MODEL:-grok-2-1212}
      - GROK_EMBEDDING_MODEL=${GROK_EMBEDDING_MODEL:-grok-v1-embedding}
      - VENICE_MODEL=${VENICE_MODEL:-llama-3.1-405b}
      - VENICE_CHUNK_MODEL=${VENICE_CHUNK_MODEL:-dolphin-2.9.2-qwen2-72b}

      # Default Providers
      - DEFAULT_EMBEDDING_PROVIDER=${DEFAULT_EMBEDDING_PROVIDER:-openai}
      - DEFAULT_CHUNK_PROVIDER=${DEFAULT_CHUNK_PROVIDER:-openai}
      - DEFAULT_SUMMARY_PROVIDER=${DEFAULT_SUMMARY_PROVIDER:-openai}

      # Application Settings
      - BATCH_SIZE=${BATCH_SIZE:-100}
      - SAMPLE_SIZE=${SAMPLE_SIZE:-2500}
      - MAX_WORKERS=${MAX_WORKERS:-4}
      - FILTER_DATE=${FILTER_DATE}
      - MAX_TOKENS=${MAX_TOKENS:-4096}
      - CHUNK_SIZE=${CHUNK_SIZE:-1000}
      - CACHE_ENABLED=${CACHE_ENABLED:-true}
      - SELECT_BOARD=${SELECT_BOARD}
      - EMBEDDING_BATCH_SIZE=${EMBEDDING_BATCH_SIZE:-500}
      - CHUNK_BATCH_SIZE=${CHUNK_BATCH_SIZE:-100}
      - SUMMARY_BATCH_SIZE=${SUMMARY_BATCH_SIZE:-100}

      # Data Paths (Fixed in container)
      - ROOT_PATH=/app/data
      - DATA_PATH=/app/data
      - KNOWLEDGE_BASE=/app/data/knowledge_base.csv
      - ALL_DATA=/app/data/all_data.csv
      - ALL_DATA_STRATIFIED_PATH=/app/data/stratified
      - PATH_TEMP=/app/temp_files
      - FILE_TYPE=${FILE_TYPE:-csv}

      # AWS Configuration
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION:-us-east-1}
      - S3_BUCKET=${S3_BUCKET}
      - S3_BUCKET_PREFIX=${S3_BUCKET_PREFIX}
      - S3_BUCKET_PROCESSED=${S3_BUCKET_PROCESSED}
      - S3_BUCKET_MODELS=${S3_BUCKET_MODELS}
    ports:
      - "${API_PORT:-5000}:5000"
    networks:
      - knowledge-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  ui:
    build:
      context: ..
      dockerfile: deployment/Dockerfile
      target: development
    volumes:
      - ../chainlit_frontend:/app/chainlit_frontend
      - ../knowledge_agents:/app/knowledge_agents
      - ../config:/app/config
      - ../data:/app/data
      - ../logs:/app/logs
      - ../pyproject.toml:/app/pyproject.toml
      - ../poetry.lock:/app/poetry.lock
      - ../.env:/app/.env
    ports:
      - "${UI_PORT:-8000}:8000"
    environment:
      # Service Type
      - SERVICE_TYPE=ui
      - DOCKER_ENV=true
      
      # Port Configuration
      - PORT=8000
      - HOST=0.0.0.0
      
      # Development Settings
      - PYTHONPATH=/app
      - LOG_LEVEL=${LOG_LEVEL:-DEBUG}

      # API Keys
      - OPENAI_API_KEY=${OPENAI_API_KEY}  # Required
      - GROK_API_KEY=${GROK_API_KEY}      # Optional
      - VENICE_API_KEY=${VENICE_API_KEY}   # Optional

      # Model Settings
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4}
      - OPENAI_EMBEDDING_MODEL=${OPENAI_EMBEDDING_MODEL:-text-embedding-3-large}
      - GROK_MODEL=${GROK_MODEL:-grok-2-1212}
      - GROK_EMBEDDING_MODEL=${GROK_EMBEDDING_MODEL:-grok-v1-embedding}
      - VENICE_MODEL=${VENICE_MODEL:-llama-3.1-405b}
      - VENICE_CHUNK_MODEL=${VENICE_CHUNK_MODEL:-dolphin-2.9.2-qwen2-72b}

      # Default Providers
      - DEFAULT_EMBEDDING_PROVIDER=${DEFAULT_EMBEDDING_PROVIDER:-openai}
      - DEFAULT_CHUNK_PROVIDER=${DEFAULT_CHUNK_PROVIDER:-openai}
      - DEFAULT_SUMMARY_PROVIDER=${DEFAULT_SUMMARY_PROVIDER:-openai}

      # Application Settings
      - BATCH_SIZE=${BATCH_SIZE:-100}
      - SAMPLE_SIZE=${SAMPLE_SIZE:-2500}
      - FILTER_DATE=${FILTER_DATE}
      - MAX_TOKENS=${MAX_TOKENS:-4096}
      - CHUNK_SIZE=${CHUNK_SIZE:-1000}
      - CACHE_ENABLED=${CACHE_ENABLED:-true}
      - SELECT_BOARD=${SELECT_BOARD}

      # Data Paths (Fixed in container)
      - ROOT_PATH=/app/data
      - DATA_PATH=/app/data
      - KNOWLEDGE_BASE=/app/data/knowledge_base.csv
      - ALL_DATA=/app/data/all_data.csv
      - ALL_DATA_STRATIFIED_PATH=/app/data/stratified
      - PATH_TEMP=/app/temp_files
      - FILE_TYPE=${FILE_TYPE:-csv}

      # AWS Configuration
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION:-us-east-1}
      - S3_BUCKET=${S3_BUCKET}
      - S3_BUCKET_PREFIX=${S3_BUCKET_PREFIX}
      - S3_BUCKET_PROCESSED=${S3_BUCKET_PROCESSED}
      - S3_BUCKET_MODELS=${S3_BUCKET_MODELS}

      # Service-specific settings
      - API_HOST=api
      - API_PORT=5000
    depends_on:
      - api
    networks:
      - knowledge-net
    restart: unless-stopped

networks:
  knowledge-net:
    driver: bridge

volumes:
  data:
  logs: