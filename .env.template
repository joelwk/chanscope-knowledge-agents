# Model Configuration
OPENAI_API_KEY=your-openai-key-here
OPENAI_MODEL=gpt-4
OPENAI_EMBEDDING_MODEL=text-embedding-3-large

GROK_API_KEY=your-grok-key-here
GROK_MODEL=grok-2
GROK_EMBEDDING_MODEL=grok-v1-embedding

VENICE_API_KEY=your-venice-key-here
VENICE_MODEL=llama-3
VENICE_CHUNK_MODEL=dolphin-2.9

# Default Provider Settings
DEFAULT_EMBEDDING_PROVIDER=openai
DEFAULT_CHUNK_PROVIDER=openai
DEFAULT_SUMMARY_PROVIDER=openai

# AWS Configuration (Optional - for data gathering)
AWS_ACCESS_KEY_ID=your-aws-key-id
AWS_SECRET_ACCESS_KEY=your-aws-secret
AWS_DEFAULT_REGION=us-east-1
S3_BUCKET=your-bucket-name
S3_BUCKET_PREFIX=data/
S3_BUCKET_PROCESSED=processed
S3_BUCKET_MODELS=models

# Application Configuration
FLASK_APP=app.py
FLASK_ENV=development
LOG_LEVEL=INFO

# Processing Parameters
MAX_TOKENS=8192
CHUNK_SIZE=1000
BATCH_SIZE=100
CACHE_ENABLED=true

# Path Settings
# Note: These paths are for local development. In Docker, they will automatically 
# be prefixed with /app by the application's path handling logic
ROOT_PATH=./data
ALL_DATA=./data/all_data.csv
ALL_DATA_STRATIFIED_PATH=./data/stratified
KNOWLEDGE_BASE=./data/knowledge_base.csv
DATA_PATH=./data
PATH_TEMP=./temp_files
FILE_TYPE=csv 